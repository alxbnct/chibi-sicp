 # -*- mode: org; -*-
 # Time-stamp: <2020-05-13 19:47:14 lockywolf>
 # Created   : [2020-05-11 Mon 21:01]
 # Author    : lockywolf gmail.com
 #+STARTUP: inlineimages
 #+STARTUP: latexpreview
 #+HTML_MATHJAX: align: left indent: 5em tagside: left font: Neo-Euler
 #+HTML_MATHJAX: cancel.js noErrors.js
 #+OPTIONS: tex:imagemagick

https://github.com/jkitchin/org-ref
https://jonathanabennett.github.io/blog/2019/05/29/writing-academic-papers-with-org-mode/
http://web.archive.org/web/20190501044840/http://www.mfasold.net/blog/2009/02/using-emacs-org-mode-to-draft-papers
https://orgmode.org/worg/org-faq.html#using-reftex-in-org-mode

This report is written as a post-mortem of a project which has,
perhaps been the largest personal project of the author: creating a
complete and wholesome solution to one of the most famous programming
problem sets in the modern computer science curricula "Structure and
Interpretation of Computer Programs" by Abelson, Sussman and Sussman.

It measures exactly:
- How much effort SICP requires (739 hours 19 minutes, 8 months, 292 sessions)
- How many computer languages it involves (6)
- How many pieces of software are required ()
- How much communication with peers is needed

It suggests:
- An applied task management software-backed procedure
- Improvements on the technical side of any hard skills teaching
- Improvements on the social side of any kind of teaching

* Introduction

Programming language textbooks are not a frequent object of study, as
they are expected to convey existing knowledge. However, teaching
practitioners, when they face the task of designing computer science
curriculum for their teaching institution, have to base their decision
on something. An "ad-hoc" teaching method, largely based on studying
some particular programming language fashionable at the time of
selection, is still a popular choice. 

However, there have been attempts to approach the design of a course
with more rigour, and the very famous "Structure and Interpretation of
Computer Programs" was born as a result of this attempt. SICP was
revolutionary for its time, and perhaps can be still considered
revolutionary nowadays. Twenty years later this endeavour was analysed
by Felleisen in a paper "Structure And Interpretation of Computer
Science Curriculum" TODO, in which he reflected upon the benefits and
drawbacks of the ~deliberately designed~ syllabus from the pedagogical
standpoint, and proposes what he believes to be a pedagogically
superior successor to the first generation of a ~deliberate~
curriculum. (How to Design Programs TODO, HTDP)

Leaving aside the pedagogical quality of the textbook (as the author
is not a practising teacher), this report touches a different (and
seldom considered!) aspect of a computer science (and in general, any
other subject's) curriculum. That is, it's how much work exactly is
required to pass a particular course.

This endeavour was spurned by the author's previous experience of
learning Partial Differential Equations using a traditional
paper-and-pen based approach, only mildly augmented with a
time-tracking software. But even such a tiny augmentation already
exposed an astonishing disparity between a declared labouriousness of
a task and the real time required to complete it. TODO (reference to
my own facebook)

The author therefore decided to build upon the previous experience and
to try and design an as smooth, manageable (the definition of
manageable reference TODO), and measurable approach to performing
university coursework, as possible. A computer science subject
therefore provided an obvious choice.

The solution was decided, broken down into parts, harnesses with a
software support system, and executed in a timely and measured manner
by the author, thus proving that the chosen goal is doable. The
complete measured data are provided. Teaching professionals may
benefit from it when planning their own coursework.

More generally, the author wants to propose a comprehensive
reassessment of university teaching in general, on the basis of
empiric approach (understanding exactly what and when every party
involved in the teaching process does), in order to select the most
efficient (potentially even using a gradient-based optimisation
algorithm) strategy when selecting a learning approach for every
particular student.


* Solution approach

 I, the author, wanted to provide a solution that would satisfy the
following principles:

 1. Be complete.
 2. Be a fairly realistic modelling of a solution process as if
executed by the intended audience of the course -- that is freshman
university students with little programming experience. 
 3. Be done in a "fully digital", "digital native" form.
 4. Be measurable.

This principles need an explanation. 

** Completeness
*** Just solve all of the exercises

I consider completeness to be the most important property of every
execution of a teaching syllabus.

In simple words, what does it mean "to pass a course" or "to learn a
subject" at all? How exactly can one formalise the statement "I know
calculus"? Or even a simpler "I have learned everything that was
implied by a university course on calculus". The following are the
potential answers to this questions as they may be given (TODO it would be
better to conduct a survey of teachers, students, employers,
politicians and random members of the community to establish what it
means _for them_):

- Passing an oral examination
- Passing a written examination
- Passing a project defence committee questioning
- Completing a required number of continuous assessment (time-limited) tasks
- Completing coursework
- Attending a prescribed number of teaching sessions (lectures and/or tutorials)
- Reading a prescribed amount of prescribed reading material
- (anything else?)

Any combination of those can be also chosen to signify the "mastering"
of a subject, but the course designer is then met with a typical
goal-attainment multi-objective optimisation problem, which are usually
still solved by reducing the multiple goals to a single engineered goal.

Now if we try to look at this with a "martian standpoint" (TODO Eric
Bern), we will see that all the goals listed above are reducible to
the "completing coursework" goal, whereas "completing coursework" is
not in the most general case reducible to any of those, so the
"engineered goal" may look essentially like a tree-structured
coursework, with possibly several tasks requiring viewing certain
video recordings and writing a response.

Moreover, thinking realistically, doing coursework is the only way
that a working professional can study without completely abandoning
her job.

Therefore, choosing a computer science textbook that is known for the
problem set that comes with it more than for the actual text of the
material was a natural choice.

But that is not enough, because "just solving all of the exercises" may be the most measurable and the most necessary learning outcome, but is it sufficient?

As I wanted to "grasp the skill" rather than just "pass the exercises", I initially decided to consider additional exercises that may be valuable as a process of "engineering a custom problem set".

One of the approaches used to "engineer exercises" is to try an put yourself in the position of a teacher, and ask yourself: "After I finish this book/problem set, will I be able to write my own book on this?".
Even if the answer is "no", continue asking yourself "Okay then. But can I at least create a book that would be conveying exactly the same now myself?".
If the answer is "yes", then learning is successful with high likelihood.

From the practical point, in the "reference solution" attached to this report, you can find exercises that are not a part of the original problem set.
Those were added by me for the very reason "to be able to reproduce the source code of the book from scratch".

*** Meta-cognitive exercises

It is often underestimated how much imbalance there is between a teacher and a pupil, in the sense that the teacher not only knows better the subject of study, that is expected, but is also effectively deciding _now_ and _when_ a student is going to study.
This is often overlooked by practitioners, how consider themselves as either just sources of knowledge, or, even worse, only the examiners.
However, it is worth considering _the whole_ effect that a teacher is having on the student's life.
In particular, a student has no other choice than to trust the teacher on the choice of exercises. 

The main point of the previous paragraph is that the teaching process is not only the process of data transmission.
It is also the process of metadata transmission, the development of meta-cognitive skills.
(TODO reference Falikman)
And therefore, meta-cognitive challenges, although they may very well be valuable contributions to the student's "thinking abilities", deserve their own share of consideration when preparing a course.

The examples of meta-cognitive exercises include:

- Non-sequentiality of material and/or exercises, when earlier ones are impossible to solve without solving later ones.
- Incompleteness of the treatise.
- Terseness of the narrative.
- Lacking of modern software support.
- Missing difficulty/hardness estimation for tasks.
- Vastly non-uniform difficulty of problems.

An additional challenge to the learning process is the lack of peer support.
There has been attempts by learning institutions to encourage peer support among the students, but the successfullness of those attempts is unclear.
Do students really help each other in those artificially created support groups?
How does side-communication, generally irrelevant to the subject of study affect the learners?

A support medium is even more important for adult self-learners, who don't get even those artificial support groups created by the school functionaries.

It should be noted that the support medium choice, no matter how irrelevant to the subject itself it may be, is a very important social factor.
This is not to say that a teacher should create a support group in that particular social medium that is fashionable at the start of the course.
This is only to say that ~deliberate effort~ should be spent on finding the best support configuration.

From the *personal experience*:

- I used the [[irc:irc.freenode.org/#scheme][#scheme freenode channel]] as a place to ask questions in real-time. #emacs was also useful.
- I used stackoverflow.com to ask offline questions.
- I used the scheme community wiki community.schemewiki.org as a reference material.
- I emailed some prominent member of the scheme community with unsolicited questions.
- I was reporting errors in the documents generated by the scheme community process.
- I was asking for help on the Chibi-scheme mailing list.
- There was also some help from the opendatascience Slack chat.
- There was also some help from the Closed-Circles data science community.
- There was also some help from the rulinux@conference.jabber.ru community.
- There was also some help from the Shanghai Linux User Group.
- There was also some help from the www.dxdy.ru scientific forum.
- There was also some help from the Haskell self-study group in Telegram.

It should be noted that out of those communities, only the OpenDataScience community, and a small Haskell community resides in a "fashionable" communication system.

The summary of the community interaction is under the "meta-cognitive" exercises section, because the skill of finding people who can help you with your problems is one of the most important soft skills ever, and one of the most hard to teach.
Moreover, naturally, the people who _can_ and _may_ answer questions are in most of the cases not at all obliged to do so, so soliciting an answer from non-deliberately-cooperating people is another cognitive exercise that is worth looking into.

I will repeat the main point of the previous paragraph in another words: human communities consist of rude people. Naturally, in the modern "free" world, no-one can force anyone to bear rudeness, but no-one can just as much force anyone to be polite.
The meta-cognitive skill of extracting valuable knowledge from willing but rude people is a very important skill.

But returning to the choice of the community, it is important to convey to the students, as well as the teachers, the following idea: it is not the fashion, population, easy availability, promotion, and social acceptability of the support media that matters.
Unfortunately, it is not even the technological advanceness, technological modernity or convenience that matters. It is the availability of information, and the availability of people who can help. This is a painful knowledge worth learning.

Support communication statistics is the following:

- Scheme interpreter related email threads: 28
- Editor/IDE related email threads + bug reports: 16
- Presentation/formatting related email threads: 20
- Syllabus related email threads: 3
- Documentation related email threads (mostly obsolete links): 16
- IRC chat messages: 2394 #scheme messages
- Software packages re-uploaded to source forges: 2

Statistics from other means is hard to collect.

** Behaviour modelling, reenactment and the choice of tools

When I started this project, I already had a Ph.D. in Informatics, although not an engineering one.
This gave me a certain advantage over a first-year undergraduate student.
However, to a large extent I resembled a newbie still, as I had never before used a proudly functional programming language, and had never used any programmer's editor other than Notepad++. The only _really_ distinguishing property of me at the start of the project was that I already had learned the skill of typing quickly and without looking at the keyboard.

*note* All of this report is _heavily_ dependent on the fact that I learned how to "touch type", and can do it relatively fast. Without the fast touch-typing (not looking at the keyboard), almost all of it has no sense, ergonomic suggestions make no sense, and the choice of tools may seem counter-intuitive or even arbitrary. 

The goal I had was slightly schizophrenic, in the sense that I intended to model (reenact) a "normal" student, that is the one that doesn't exist, in the sense that I:

- Decided to perform all exercises honestly, no matter how hard they be or how much time they take.
- Solve all exercises myself. Although that didn't restrict me on consulting other people's solution when this didn't involve direct copying.
- Try to use the tools that may have been available at the disposal of the students in 1987, although possibly the most recent versions.
- Try to follow the "Free Software/Open Source/Unix Way" approach as loosely formulated by the well known organisations, as close as possible. 
- Try to prepare a "problem set solution" in a format that may be potentially presentable to a university teacher in charge of accepting or rejecting it.

While the first three principles turned out to be almost self-fulfilling, the last one turned out to be more involved.

My own personal experience with the university-level programming suggested than on average the largest amount of time is spent on debugging input and output procedures.
The second-largest amount is usually dedicated to inventing test cases for the code.
The actual writing of the substantive part of the code only comes the third.

As I knew that SICP had been intended as a deliberately created introductory course, I assumed that a large part of the syllabus would be dedicated to solving the two most laborious problems.
I was wrong.
Rather than solving them, SICP just goes around, enforcing  a very rigid standard on the input data instead.

The final choice of tools turned out to be the following:

- chibi-scheme :: as it is the scheme implementation
- GNU Emacs :: as the only IDE
  - org-mode :: as the main editing mode and the main planning tool
  - f90-mode :: as a low-level coding adaptor
  - geiser :: turned out to be not really ready for production use
  - magit :: as the most fashionable GUI for git
- gfortran :: as the low-level language
- PlantUML :: as the principal diagramming language
- Tikz + luaLaTeX :: as the secondary diagramming language
- graphviz :: as a tertiary diagramming language
- imagemagick :: as the engine behind the "picture language" chapter
- git :: as the main version control tool
- GNU diff, bash, grep :: as the tools for simple text manipulation

The choice of all the software above except "org-mode" is driven by the "imitative approach".
That is, I tried to imagine myself being an "ideal student" and making the decisions as the imaginary student would be doing them.
Informally this can be summarised as "I will learn every tool that is required to get the job done to the extent needed to get the job done, but not a slightest bit more".

*chibi-scheme* is effectively the only scheme system claiming to support the last scheme standard, r7rs-large (Red Edition), so there was really no other choice.
This is especially true when imagining a student unwilling to go deeper into the particular curiosities of various schools of thought creating various partly-compliant scheme systems.

*git* is not often taught in schools.
Maybe because the teachers don't want to busy themselves with something deemed trivial or impossible to get by without, or due to being overloaded with work.
However, practice often demonstrates that students still too often graduate without yet having a concept of file version control, which significantly hinders work efficiency.
I chose git, because it is, arguably, the most widely used version control system.

*imagemagick* turned out to be the easiest way to draw simple straight line based images from scheme.
There is still no standard way to connect scheme applications to applications written in other languages.
Therefore, by the principle of minimal extension, imagemagick was chosen, as it required ~just a single~ non-standard scheme procedure.
Moreover, this procedure (a simple synchronous application call) is likely to be the most standard interoperability primitive invented.
Almost all operating systems support applications executing other applications.

*PlantUML* is a text-based implementation of the international standard of software visualisation diagrams. 
The syntax is very easy, well documented.
The PlantUML-Emacs interface exists and is relatively reliable.
The textual representation conveys the hacker spirit, and supports easy version control.
UML almost totally dominates the software visualisation market, and almost every university programming degree includes it to some extent.
It seemed therefore very natural to (where the problem permitted) solve the "diagramming" problems of the SICP with the industry standard compliant diagrams.

*graphviz* was used in an attempt to use another industry standard for solving those diagramming problems unsupported by the UML.
The ~dot~ package benefits from being fully machine-parseable and context independent even more than UML. However, it turned out to be not as convenient as expected. 

*TikZ* is essentially the only general-purpose text-based drawing package.
So when neither UML nor DOT managed to properly embed the complexity of the models diagrammed, TikZ ended up being the only choice.
Just as natural of an approach could be drawing everything with a graphical tool, such as Inkscape or Adobe Illustrator.
The first problem with the images generated by them is though that those are hard to manage under version control.
The second problem is that (I will get to it later) for the purposes of easy defendability of the resulting work, it was desirable to keep all the product of the course in one digital artefact (read, one file).

*gfortran*, or GNU Fortran was the low language of choice for the last two problems in the problem set.
The reason for choosing not a very popular language were the following:
- I already knew the C language, so compared to an imaginary first year student I would have an undue advantage.
- Fortran is low-level enough for the purposes of the book.
- There is a free/GPL implementation of Fortran.
- Fortran 90 had already existed by the moment SICP 2nd. Ed. was released.

*GNU Unix Utilities* I didn't originally intend to use, but ~diff~ turned out to be extremely effective in illustrating the difference between generated code pieces in the Chapter 5. Additionally, bash printf had to be used as a bug work-around.

*GNU Emacs*: is de-facto the most popular IDE among scheme users, the IDE used by the Free Software Foundation founders, likely the editor used when writing SICP, also likely to be chosen by an aspiring freshman to be the most "hacker-like" editor.
It is, perhaps, the most controversial choice, as the most likely IDE to be used by freshmen university students in general would be Microsoft Visual Studio.
Another popular option would be Dr.Racket.
However, at the end of the day, Emacs turned out to be having the most superior support for a "generic Lisp" development, even though it's support for scheme is not as good as may be desired.
The decisive victory point actually ended up being the org-mode (discussed later).
Informally speaking, fully buying into the Emacs platform ended up being a huge mind-expanding experience.
The learning curve is steep though.
As I mentioned above, the main point of this report is to supply the problem execution telemetry for public use.
Later I will elaborate on how I collected it, however I can already say that I use org-mode's time tracking facility. However, I had learned Emacs in general before I learned org-mode, and thus only the Emacs Lisp part got covered by time management.

But already here I can list some *data*:
Just reading the Emacs Lisp manual required *10* study sessions of total length 32 hours 40 minutes.
Additional learning of Emacs *without* reading the manual required 59 hours 14 minutes.

This data will be also presented in a table later. TODO

*org-mode*
Imagine a case when a student needs to send his work to the teacher for examination.
Every additional file that a student sends along with the code is a source of confusion.
Even proper file naming, though increases readability, is hard to enforce, and demands that the teacher dig into the peculiarities that will become irrelevant the very moment after he signs the work off.
Things get worse when the teacher has to not only examine the student's work, but also test it.
(Which is a common case with computer science exercises.)

SICP also provides and additional challenge (meta-cognitive exercise) in that its problems are highly dependent on one another.
As an example, problems from Chapter 5 require successfully completed exercises of Chapter 1.
A standard practice of modern schools is to copy the code (or other forms of solution).
However, in the later parts of SICP, the solutions end up requiring up to tens of pieces of code written in the chapters before.
Sheer copying would not just blow up the solution files immensely and make searching painful.
It would also make it extremely hard to back-propagate the bugs discovered by later usages into the earlier solutions.

The third reason to carefully consider the solution format is the future employability of the students.
This problem is not uncommon for the Arts majors, who have been garnering "portfolios" of their work since ages ago.
But this feeling is still generally lacking among technical students.
One of the great discussion subjects on a job interview is "what have you done".
And having a portfolio is of an immense help for the interviewee.

But the potential employer is almost guaranteed to not have any software or equipment to run the former student's code.
And in fact even the student himself would probably be lacking the carefully prepared working setup at the interview.
Therefore, the graduation work should be "stored", or "canned" in an portable and time-resistant format as possible.

Unsurprisingly, the most portable and time-resistant format of practical usage is plain white paper.
So ideally the solutions (after being examined by a teacher) should be printable in the form of a report or a book.
Additionally, the comparatively (to the full size of SICP) small amount of work required to turn a solution that is "just enough to pass" into a readable report would be an important emotional incentive for the students to carefully post-process their work.
Naturally, "plain paper" is not a very manageable medium nowadays.
But the closes, and quite manageable approximation is PDF.
So the actual "source code" of a solution should be logically and consistently exportable into PDF.

This leads us to the idea first proposed by Donald Knuth with his WEB system and its web2c implementation.
The implementation of WEB for Emacs is called org-mode, in particular with its org-babel module.
Another commonly used WEB implementation is called Jupyter.

Org-mode has an almost unimaginable number of use cases.
(In particular, this report has been written in org-mode.)
And while the main benefit of using org-mode for the coursework formatting was the interactivity of code execution, and the possibility of export, another benefit that appeared almost for free was minimal-overhead time-tracking.
(Human performance profiling.)
Although it originally appeared as a by-product of choosing a specific tool, at the end of the day it is the telemetry collected with the aid of it, that is the main contribution of this report.

The way org-mode particulars were used is described in the next section, along with the statistical summary.

** TODO Time analysis, performance profiling and graphs

The execution was performed in the following way: 

Firstly, the heading outline corresponding to the book subsection tree was created.
(See Appendix 1 for the outline and completion times.)
Most leaves are two-state TODO-headings.
(Some leaves correspond to sections without problems, and thus are not TODO-styled.)

Intermediate levels are not TODO-headings, but they contain the field representing the total ratio of DONE problems.

The top level ratio, obviously, looks like the ratio of the total number of finished problems versus the total number of problems.

This allows for constant monitoring of the "degree of completeness" and provides an important emotion of "getting close to the result with each complete exercise".
Additional research is needed on how persistent this emotion is in students and how much it depends of the uneven distribution of hardness or time consumption.
There is, however, empirical evidence that even very imprecise self-measured KPIs do positively affect the chance of reaching the goal.
(TODO reference "the people who measure their weight more often are more fit") 
It should be noted though that even if the hypothesis of uneven time consumption affects the positively stimulating emotion, the problems we find in the real world are not evenly hard, and therefore an even distributions of hardness may negatively affect the development of the meta-cognitive still of partitioning a task in smaller ones.

The problems were executed almost sequentially, and the work on the next one was started immediately after the previous one had been finished.
Deliberate effort was spent on avoiding the cases when a study session ends at the same time as the last problem of the session is done.
This was done in order to exploit the well-known tricks: 

- When you have something undone, it is easier to make yourself start the next session.
- Even just reading out the description of a problem makes you start thinking about how to solve it.

Exercise completion time was registered with a standard org-mode completion time mechanism.

Study sessions were registered in a separate org-mode file in the standard org-mode time interval standard BEGIN_TIME -- END_TIME.
(Appendix 2)




** Materials

*** Books 
- Structure and Interpretation of Computer Programs 2nd Ed.
- Structure and Interpretation of Computer Programs 1st Ed. pre-print
- Modern Fortran Explained 2018
- Revised^7 Report on Algorithmic Language Scheme
- Balbin, I., Lecot, K. (Eds.) Logic Programming: A Classified Bibliography
- Chibi-scheme manual (improvised)
- TikZ manual
- PlantUML manual
- Thomas A. Pender-UML Weekend Crash Course

*** Software
- GNU Emacs
- org-mode for Emacs
- chibi-scheme
- MIT/GNU Scheme
- luaLaTeX/TexLive!
- TikZ/PGF
- PlantUML
- Graphviz
- Slackware Linux 14.2-current

